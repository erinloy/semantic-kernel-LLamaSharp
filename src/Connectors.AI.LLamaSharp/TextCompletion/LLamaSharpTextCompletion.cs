using Connectors.AI.LLamaSharp.TextCompletion;
using LLama;
using Microsoft.SemanticKernel.AI.TextCompletion;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading;
using System.Threading.Tasks;

namespace Microsoft.SemanticKernel.Connectors.AI.LLamaSharp.TextCompletion;

// TODO: This is a copy of the ChatCompletion version.  It needs to be updated to use the TextCompletion version of the LLamaSharp model.

/// <summary>
/// Text Completion use LLamaSharp
/// </summary>
public class LLamaSharpTextCompletion : ITextCompletion, IDisposable
{
    readonly ChatSession<LLamaModel> _session;
    readonly LLamaModel _model;
    private bool isDisposed;

    /// <summary>
    /// Create LLamaSharpTextCompletion Instance
    /// </summary>
    /// <param name="modelPath"></param>
    /// <param name="prompt"></param>
    /// <param name="antiPrompt"></param>
    public LLamaSharpTextCompletion(string modelPath, string prompt, string[] antiPrompt = null)
    {
        this._model = new LLamaModel(new LLamaParams(model: modelPath));
        this._session = new ChatSession<LLamaModel>(this._model).WithPrompt(prompt).WithAntiprompt(antiPrompt);
    }

    /// <summary>
    /// Get completion results for the prompt and settings.
    /// </summary>
    /// <param name="text">The prompt to complete.</param>
    /// <param name="requestSettings">Request settings for the completion API</param>
    /// <param name="cancellationToken">The <see cref="CancellationToken"/> to monitor for cancellation requests. The default is <see cref="CancellationToken.None"/>.</param>
    /// <returns>List of different completions results generated by the remote model</returns>
    public Task<IReadOnlyList<ITextCompletionResult>> GetCompletionsAsync(
        string text,
        CompleteRequestSettings requestSettings,
        CancellationToken cancellationToken = default)
    {
        return Task.FromResult(new List<ITextCompletionResult>() { new LLamaTextCompletionResult(string.Join("", _session.Chat(text))) } as IReadOnlyList<ITextCompletionResult>);
    }

    /// <summary>
    /// Get streaming completion results for the prompt and settings.
    /// </summary>
    /// <param name="text">The prompt to complete.</param>
    /// <param name="requestSettings">Request settings for the completion API</param>
    /// <param name="cancellationToken">The <see cref="CancellationToken"/> to monitor for cancellation requests. The default is <see cref="CancellationToken.None"/>.</param>
    /// <returns>List of different completion streaming results generated by the remote model</returns>
    public async IAsyncEnumerable<ITextCompletionStreamingResult> GetStreamingCompletionsAsync(
        string text,
        CompleteRequestSettings requestSettings,
        CancellationToken cancellationToken = default)
    {
        yield return new LLamaTextCompletionResult(_session.Chat(text));
    }

    /// <summary>
    /// Dispose
    /// </summary>
    /// <param name="disposing"></param>
    protected virtual void Dispose(bool disposing)
    {
        if (!isDisposed)
        {
            if (disposing)
            {
                _model.Dispose();
            }
            isDisposed = true;
        }
    }

    /// <inheritdoc/>
    public void Dispose()
    {
        // Do not change this code. Put cleanup code in 'Dispose(bool disposing)' method
        Dispose(disposing: true);
        GC.SuppressFinalize(this);
    }
}
